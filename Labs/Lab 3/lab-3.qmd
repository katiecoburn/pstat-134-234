---
title: "Lab 3"
author: "PSTAT 134/234"
format: html
editor: visual
---

## Web Scraping

As we'll talk about in lecture, it's important to scrape websites consciously, with minimal negative impact. Every time you load a website, you're technically making a request to that server. As an individual person browsing the Web, there's only so many requests you will likely make in a given period of time, even if you visit a lot of websites or refresh pages a few times. But with a scraper that could potentially execute thousands of requests a second, you could end up costing the Web site owner a **lot of money** and even possibly **bring down their site** (this is called a [Denial-of-Service attack, or DoS](https://en.wikipedia.org/wiki/Denial-of-service_attack)).

You also want to make sure that it is legal to scrape a specific Web site at all.

Two general rules that we will [**always**]{.underline} follow are:

1.  Make only one request per page;
2.  Wait a little between each request.

## Data to Scrape

For this week's lab, we'll work with this Web page: <https://www.allsides.com/media-bias/ratings>

AllSides is an interesting site that attempts to make the political leanings of different media sources more transparent. Their ratings page displays a number of different news sources, along with their overall rating of the source, on a scale from "Left" to "Right," and the proportion of members of the public who agree with that rating.

This is especially relevant in light of our upcoming election – and this page provides some unique challenges, so it should be a great candidate for an example.

![Screengrab of AllSides ratings page.](images/Screenshot%202024-10-13%20at%203.14.49%20PM.png)

Before scraping any web site, you'll want to investigate their Terms of Service (sometimes called Terms of Use) and their `robots.txt` file. The Terms of Use page for this site makes no discernible mention of web scraping (see here: <https://www.allsides.com/terms-of-use>). To double-check, you can search the page for terms related to web scraping or crawling. Nothing pops up, so we're good there. Next, we check `robots.txt`. Here's theirs: <https://www.allsides.com/robots.txt> The page that we want to scrape isn't listed under any of their "Disallow" sections. We should note that `Crawl-delay: 10`, which means they request that anyone using a web crawler should wait 10 seconds between requests. Otherwise, we are good to go!

Let's do the actual scraping. Remember, be very careful to access the page (run `read_html`) **only once**. If you try to access it more than once, it may actually stop working and give you an error (often Error 403; look back at our API notes to find what that code stands for).

Run `read_html` **once**, then store the results. My code to do so is as follows:

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# install.packages("rvest")
library(rvest)
# install.packages("xml2")
library(xml2)

# html <- read_html("https://www.allsides.com/media-bias/ratings")
# Notice that we store this so we ONLY REQUEST THE PAGE ONCE.
# I save the results and load it back in so that I don't re-request
# every time I render.

# write_html(html, file = "lab-3-data.html")
html <- read_html(x = "lab-3-data.html")
```

## Scraping in R: `rvest`

First we need to find where and how the data we want are structured inside the HTML for our Web page. Unfortunately, every Web page is likely to be at least slightly different. Some will use tables, some won't; some will use JavaScript, some won't; some will use frames, some won't. The more complex the structure of the Web page, the harder it may ultimately be to scrape. This is why, if a company or organization has a publicly available API, it's almost always easier and more efficient to access their data using their API.

Not every site **has** an API, however. Sometimes the information you want is just not directly downloadable – and copying and pasting text, which is a sort of manual "web scraping," in a sense, is very slow and prone to errors. So we turn to web scraping.

Most browsers (Safari, Chrome, Firefox, etc.) are kind enough to provide a relatively quick way of finding the selector for an individual element using their developer tools. In Google Chrome, you can simply right-click on the element of interest, then click on `Inspect`. When you do, the developer tools pane will appear, highlighting the element you clicked on.

If I highlight the entire table, for example, and then `Inspect`, I see the following:

![](images/Screenshot%202024-10-13%20at%204.10.12%20PM.png)

This tells us that all the news source data is neatly stored inside one big table. That makes our life much easier. Each row's information is stored within the `<tr></tr>` tags within `<tbody></tbody>`. We can look at a single row like this:

![](images/Screenshot%202024-10-13%20at%204.11.04%20PM.png)

So to extract the data for each source, we just need to select all the `<tr>` tags within `<tbody>`. In R, we can do this by:

```{r}
table_rows <- html %>% 
  html_elements("tbody") %>% 
  html_elements("tr")
```

Now we have a set of 50 HTML table rows (one per source) that each contain four cells:

-   News source name and link

-   Bias data

-   Agreement buttons

-   Community feedback data

Let's break down how we could extract all this information. First of all, the **news source name**. If we highlight the first name and `Inspect`, we see that:

![](images/Screenshot%202024-10-13%20at%204.22.31%20PM.png)

The outlet name – `ABC News (Online)` – is the text of an **anchor tag** (the `<a href>`), which is nested inside a `<td>` tag. We don't just want to ask for all `<td>` tags, because that would return almost everything, not just the names. Instead, we can ask for all the HTML elements with `.source-title`, which pulls out all the `<td>` tags whose class contains `source-title`.

We then pass those elements to `html_text2()`, which drops the `<td>` and `<a href>` tags and simply retains the text of the source titles:

```{r}
source_titles <- table_rows %>% 
  html_elements(".source-title") %>% 
  html_text2()
```

We also might want to obtain the link for each news source's page on AllSides. We can pull that out while we're here; it's also in the `<td>` tags with `source-title`.

```{r}
source_links <- paste0("https://www.allsides.com", 
                       (table_rows %>% 
                          html_elements(".source-title") %>% 
                          html_elements("a") %>% 
                          html_attr("href")))
```

Notice that this time we need to ask for the `<a>` elements, and then, rather than simply extracting the text, we want the attribute value that is assigned to `href`, which is the text to our links.

Each source's link is actually just what is called a relative path; for example, rather than <https://www.allsides.com/news-source/forbes>, the link is simply [/news-source/forbes](/news-source/forbes). That works if you're on the AllSides page already, but if we just wanted to get to that page directly from anywhere, it wouldn't work. That's why we use `paste0` to add the first part of the URL to each source's link.

Now let's look at the bias rating. Well, this seems tricky. The bias rating information is stored in an image! How can we extract that information in words?

Take a look at the HTML code for one of the bias rating images:

![](images/Screenshot%202024-10-13%20at%204.32.15%20PM.png)

The image is linked inside this `<td class>` tag, and the **image** itself is wrapped inside another link tag, an `<a href>` tag! The best part is that this link goes to `/media-bias/left-center`. You can click on some of the bias ratings to verify. Each rating points to a link about the definition of that category; in other words, sources rated `left-center` link to that page, sources rated `center` link to **that** page, and so on. This means that we can extract the bias rating information itself simply by accessing that link tag.

```{r}
source_bias <- table_rows %>% 
  html_elements(".views-field-field-bias-image") %>%
  html_elements("a") %>%
  html_attr("href") %>% 
  str_remove("/media-bias/")
```

You can run each line there one at a time to verify what we're doing. We essentially pull out the relative path from each link, then drop the first part of the path (the `"/media-bias/"`) to retain only the last part. Each source now has a value of `left-center`, `left`, `center`, `right-center`, etc., accordingly.

The last thing we might want is the community feedback data – how many people agreed vs. disagreed with the rating, and the judgment (strongly agrees, absolutely agrees, etc.). This information is certainly more convoluted. The number of people who agree and disagree is, at least, relatively easy to grab, because their tags have classes unique to that cell, `"agree"` or `"disagree"`.

![](images/Screenshot%202024-10-13%20at%204.46.43%20PM.png)

We can pluck them out, read the text, and convert them to numbers as follows:

```{r}
source_agree <- table_rows %>% 
  html_element(".agree") %>%
  html_text2() %>% 
  as.numeric()

source_disagree <- table_rows %>% 
  html_element(".disagree") %>%
  html_text2() %>% 
  as.numeric()
```

We also might want the ratio of agree to disagree:

```{r}
source_agree_ratio <- source_agree/source_disagree
```

Now we just have the last part, the text saying whether the community agrees, strongly agrees, etc. That might seem like the easiest part to extract, but you'll find it is surprisingly difficult. If you just run:

```{r}
table_rows %>% 
  html_elements(".commtext")
# nothing!
# because that part is rendered with javascript and would
# require special techniques to extract
# so we can do some digging into their javascript (.js) files
# and find that they have a function that calculates the text with
# these parameters:
```

You get absolutely nothing. Why? Because that text is rendered with JavaScript. There are certain tools for web scraping that allow you to extract information rendered with JavaScript, although that takes a little more work. In this case, since this is the only piece of information rendered that way, we can manually recreate it. How can we do that? Well, first we need to figure out what AllSides' criteria are for determining whether the community agrees, strongly agrees, etc.

You can do a search (CTRL-F) for `"js"` in the file source to find all the JavaScript files they're using and open the files in a new tab to look for their rules. Note that this is more work than I would ask you to do on your homework assignment, for example, as the rules were in their **eleventh** JavaScript file. They have a function that calculates the text based on the ratio of agree to disagree, as follows:

| Range                   | Agreeance            |
|-------------------------|----------------------|
| $ratio > 3$             | absolutely agrees    |
| $2 < ratio \leq 3$      | strongly agrees      |
| $1.5 < ratio \leq 2$    | agrees               |
| $1 < ratio \leq 1.5$    | somewhat agrees      |
| $ratio = 1$             | neutral              |
| $0.67 < ratio < 1$      | somewhat disagrees   |
| $0.5 < ratio \leq 0.67$ | disagrees            |
| $0.33 < ratio \leq 0.5$ | strongly disagrees   |
| $ratio \leq 0.33$       | absolutely disagrees |

So we can replicate that in R:

```{r}
get_agreement_text <- function(ratio) {
  case_when(
    ratio > 3 ~ "absolutely agrees",
    ratio > 2 & ratio <= 3 ~ "strongly agrees",
    ratio > 1.5 & ratio <= 2 ~ "agrees",
    ratio > 1 & ratio <= 1.5 ~ "somewhat agrees",
    ratio == 1 ~ "neutral",
    ratio > 0.67 & ratio < 1 ~ "somewhat disagrees",
    ratio > 0.5 & ratio <= 0.67 ~ "disagrees",
    ratio > 0.33 & ratio <= 0.5 ~ "strongly disagrees",
    ratio <= 0.33 ~ "absolutely disagrees"
  )
}

# Let's verify that the function works:
get_agreement_text(1)
```

Now let's lay all that code out and make a data frame (tibble) of the first 50 rows – that whole page:

```{r}
source_titles <- table_rows %>% 
  html_elements(".source-title") %>% 
  html_text2()
source_links <- paste0("https://www.allsides.com", 
                       (table_rows %>% 
                          html_elements(".source-title") %>% 
                          html_elements("a") %>% 
                          html_attr("href")))
source_bias <- table_rows %>% 
  html_elements(".views-field-field-bias-image") %>%
  html_elements("a") %>%
  html_attr("href") %>% 
  str_remove("/media-bias/")
source_agree <- table_rows %>% 
  html_element(".agree") %>%
  html_text2() %>% 
  as.numeric()
source_disagree <- table_rows %>% 
  html_element(".disagree") %>%
  html_text2() %>% 
  as.numeric()
source_agree_ratio <- source_agree/source_disagree
source_agreement <- get_agreement_text(source_agree_ratio)

data = tibble(source_titles, source_links, source_bias,
              source_agree, source_disagree, source_agree_ratio,
              source_agreement)
data
```

How could you scale this up? Well, imagine that there were multiple pages of sources and ratings. If each page had the same structure, and the URLs followed some kind of consistent pattern – for example, if the second page had the URL <https://www.allsides.com/media-bias/ratings?page=2>, and so on – you could simply write a loop, like so, to extract information from multiple pages:

```{r, eval=FALSE}
urls <- c("https://www.allsides.com/media-bias/ratings",
          "https://www.allsides.com/media-bias/ratings?page=2",
          "https://www.allsides.com/media-bias/ratings?page=3")
for(i in 1:length(urls)){
  html <- read_html(urls[i])
  table_rows <- html %>% 
    html_elements("tbody") %>% 
    html_elements("tr")
  extract_data(table_rows) # Notice this is not a real function;
  # instead you would run whatever code you'd created to extract the data.
  Sys.sleep(10) # to wait 10 seconds between requests
}
```

Note that I set `eval=FALSE` to the above code chunk; AllSides doesn't follow that URL structure, so this is a hypothetical example to demonstrate how the process would work for a website that was organized this way.

Now let's take a look at the data that we scraped!

### Exercises

1.  Find all the sources on which the community **absolutely agrees**. Display the titles of these sources and their bias. How many "center" sources does the community absolutely agree on?
2.  Which sources do the community **disagree** on? (Consider "somewhat disagrees" to "absolutely disagrees".)
3.  Make a bar chart of community agreeance. Does the community tend to agree or disagree with AllSides' ratings?
4.  Make a table of bias. What do you notice?
5.  Study the following plot. What patterns do you notice? Pay attention to each line of code and see if you can figure out what each part does.

```{r}
data %>%
  pivot_longer(cols = c(source_agree, source_disagree), names_to = "feedback", values_to = "count") %>%
  group_by(source_titles) %>%
  mutate(proportion = count / sum(count)) %>%
  mutate(agree_proportion = sum(count[feedback == "source_agree"]) / sum(count)) %>%
  ungroup() %>%
  mutate(source_titles = reorder(source_titles, agree_proportion)) %>% 
  ggplot(aes(y = source_titles)) +
  geom_bar(aes(x = proportion, fill = factor(feedback)), position = "fill",
           stat = "identity") +
  guides(fill = guide_legend(title = NULL)) +
  scale_fill_manual(values = c("#5DAF83", "#AF3B3B"), labels = c("Agree", "Disagree")) +
  labs(x = "Percentage", y = "") +
  theme(axis.text.y = element_text(size = 8, angle = 10)) +
  scale_x_continuous(labels = scales::percent, limits = c(0, 1))

```

## Scraping in Python: `Beautiful Soup`

If you are wanting to use Python to build your web scraper, there's no library I would recommend more than `Beautiful Soup`. Read more about it here: <https://www.crummy.com/software/BeautifulSoup/> You can also find their "Hall of Fame," a selection of projects made with the software.

`BeautifulSoup` was created in 2004. Its name is meant to be a reference to both the term "tag soup," which refers to poorly-organized HTML code, and the poem "Beautiful Soup," from [Alice in Wonderland]{.underline}. The implication is that this Python library enables you to take messy, disorganized data (tag soup) and recognize that it is actually a beautiful soup, because you have the ability to parse it efficiently and obtain organized information.

![Snippet of text from the poem by Lewis Carroll, along with original illustration.](images/Screenshot%202024-10-13%20at%207.07.36%20PM.png)

We're going to switch to some Python code chunks now. This will require loading the necessary Python libraries. If you haven't already installed `BeautifulSoup`, you'll need to uncomment and run the code in the following R code chunk first:

```{r, message=FALSE, warning=FALSE}
library(reticulate)
# py_install("beautifulsoup4")
```

```{python}
import requests
from bs4 import BeautifulSoup
```

Note that I will set the following code chunk to not evaluate – why? Because I have already requested the content of the page we're scraping above, and stored it to an .html file. Remember that we **only want to request a page once**, as a rule. Scrape politely!

```{python, eval=FALSE}
url = 'https://www.allsides.com/media-bias/media-bias-ratings'

r = requests.get(url)

soup = BeautifulSoup(r.content, 'html.parser')
```

Instead, we'll load the .html file that was saved earlier:

```{python}
htmlfile = open("lab-3-data.html", "r") 
  
# Reading the file 
index = htmlfile.read() 
  
# Creating a BeautifulSoup object and specifying the parser 
soup = BeautifulSoup(index, 'html.parser')
```

We'll follow the same steps as we did with the data in R using `rvest`. First, we'll pull out the rows in the table containing the information we want:

```{python}
rows = soup.select('tbody tr')
```

`tbody tr` tells the selector to extract all `<tr>` tags that are children of (inside of) the `<tbody>` tag.

Then we'll pull out the news source names. In Python, it might be easier to do this for the first row initially, and then progress to all the rows once we have the code figured out. For the first row, we can pull out the source name the same way we did in R; we can get the first row, select `.source-title`, and extract the text.

```{python}
row = rows[0]

name = row.select_one('.source-title').text.strip()

print(name)
```

For the news source link, it will be very similar; we add the `a`, select the `href` attribute, and attach the first part of the URL, "<https://www.allsides.com>":

```{python}
allsides_page = row.select_one('.source-title a')['href']
allsides_page = 'https://www.allsides.com' + allsides_page

print(allsides_page)
```

For the bias information, we'll again pull it out from the link URL:

```{python}
bias = row.select_one('.views-field-field-bias-image a')['href']
bias = bias.split('/')[-1]

print(bias)
```

We select the anchor tag by using the class name and tag together: `.views-field-field-bias-image` is the class of the `<td>` and `a` is for the anchor nested inside. We extract `href` like before, but now we only want the last part of the URL for the name of the bias, so we split on the slashes (`/`) and retain the last element of that split (here, `left-center`).

Then the community feedback information:

```{python}
agree = row.select_one('.agree').text
agree = int(agree)

disagree = row.select_one('.disagree').text
disagree = int(disagree)

agree_ratio = agree / disagree

print(f"Agree: {agree}, Disagree: {disagree}, Ratio {agree_ratio:.2f}")
```

Using `.text` returns a string, so we need to convert to integers – just like we used `as.numeric()` in R. On a side note – if you've never seen this way of formatting print statements in Python, the `f` at the front allows you to insert variables directly into the string using curly braces. The `:.2f` is a way to format a float to show only two decimal places.

We can again see that extracting the "somewhat agree" text is going to be a little trickier:

```{python}
print(row.select_one('.community-feedback-rating-page'))
```

And we'll make a function to replicate their rules for calculating the text:

```{python}
def get_agreeance_text(ratio):
    if ratio > 3: return "absolutely agrees"
    elif 2 < ratio <= 3: return "strongly agrees"
    elif 1.5 < ratio <= 2: return "agrees"
    elif 1 < ratio <= 1.5: return "somewhat agrees"
    elif ratio == 1: return "neutral"
    elif 0.67 < ratio < 1: return "somewhat disagrees"
    elif 0.5 < ratio <= 0.67: return "disagrees"
    elif 0.33 < ratio <= 0.5: return "strongly disagrees"
    elif ratio <= 0.33: return "absolutely disagrees"
    else: return None
    
print(get_agreeance_text(1))
```

Now that we have all the general code for a specific row, we can create a loop to extract data from every row on the page:

```{python}
data= []

for row in rows:
    d = dict()
    
    d['name'] = row.select_one('.source-title').text.strip()
    d['allsides_page'] = 'https://www.allsides.com' + row.select_one('.source-title a')['href']
    d['bias'] = row.select_one('.views-field-field-bias-image a')['href'].split('/')[-1]
    d['agree'] = int(row.select_one('.agree').text)
    d['disagree'] = int(row.select_one('.disagree').text)
    d['agree_ratio'] = d['agree'] / d['disagree']
    d['agreeance_text'] = get_agreeance_text(d['agree_ratio'])
    
    data.append(d)
```

Our `data` list now contains a big JSON dictionary containing the key-value pairs of information for every row that we scraped:

```{python}
print(data[0])
```

Again, this is only the first page. In a **hypothetical example** where there were multiple pages with organized URLs, you could run the following chunk of code to scrape **three** pages. Note that we **import the `sleep`** **function**. This is to ensure that we wait 10 seconds between requests to the server, as they specify in their `robots.txt`.

```{python, eval=FALSE}
pages = [
    'https://www.allsides.com/media-bias/media-bias-ratings',
    'https://www.allsides.com/media-bias/media-bias-ratings?page=1',
    'https://www.allsides.com/media-bias/media-bias-ratings?page=2'
]

from time import sleep

data= []

for page in pages:
    r = requests.get(page)
    soup = BeautifulSoup(r.content, 'html.parser')
    
    rows = soup.select('tbody tr')

    for row in rows:
        d = dict()
        
        d['name'] = row.select_one('.source-title').text.strip()
        d['allsides_page'] = 'https://www.allsides.com' + row.select_one('.source-title a')['href']
        d['bias'] = row.select_one('.views-field-field-bias-image a')['href'].split('/')[-1]
        d['agree'] = int(row.select_one('.agree').text)
        d['disagree'] = int(row.select_one('.disagree').text)
        d['agree_ratio'] = d['agree'] / d['disagree']
        d['agreeance_text'] = get_agreeance_text(d['agree_ratio'])

        data.append(d)
    
    sleep(10)
```

Now we can finally take our data, in JSON format, and use `pandas` to turn it into a data frame for easier use (just like we did in the API section):

```{python}
import pandas as pd

df = pd.DataFrame(data)

df.set_index('name', inplace=True)

df.head()
```

### Exercises

6.  Study the following plot. What patterns do you notice? Pay attention to each line of code and see if you can figure out what each part does.

```{python}
import matplotlib.pyplot as plt

df['total_votes'] = df['agree'] + df['disagree']

df3 = df.copy()

fig = plt.figure(figsize=(15,15))

biases = df3['bias'].unique()

for i, bias in enumerate(biases):
    temp_df = df3[df3['bias'] == bias].iloc[:10]
    temp_df.sort_index(inplace=True)
    
    max_votes = temp_df['total_votes'].max()
    
    ax = fig.add_subplot(3, 2, i + 1)
    
    ax.bar(temp_df.index, temp_df['agree'], color='#5DAF83')
    ax.bar(temp_df.index, temp_df['disagree'], bottom=temp_df['agree'], color='#AF3B3B')
    
    for x, y, ratio in zip(ax.get_xticks(), temp_df['total_votes'], temp_df['agree_ratio']):
        ax.text(x, y + (0.02 * max_votes), f"{ratio:.2f}", ha='center')
    
    ax.set_ylabel('Total feedback')
    ax.set_title(bias.title())
    
    ax.set_ylim(0, max_votes + (0.12 * max_votes))
    
    plt.setp(ax.get_xticklabels(), rotation=30, ha='right')
    
plt.tight_layout(w_pad=3.0, h_pad=1.0)
plt.show()
```

## References

This lab has referenced the following:

-   [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)

-   [Chapter 24: Web Scraping](https://r4ds.hadley.nz/webscraping)

-   [Ultimate Guide to Web Scraping With Python](https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/)
